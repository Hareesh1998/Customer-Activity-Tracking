📡 Real-Time Customer Activity Tracking System

This project demonstrates an end-to-end **Big Data pipeline** built to track, process, and visualize real-time user activity (like clicks and product views) from an e-commerce platform. Using **Apache Kafka** for ingestion, **Databricks + Delta Lake** for stream processing, and **Tableau** for reporting, this system provides live dashboards for monitoring customer behavior and trends.

---

## 🛠️ Tech Stack

| Layer             | Tools / Technologies                    |
| ----------------- | --------------------------------------- |
| Data Simulation   | Python                                  |
| Ingestion         | Apache Kafka                            |
| Stream Processing | Databricks (Spark Structured Streaming) |
| Storage           | Delta Lake (Databricks)                 |
| Visualization     | Tableau                                 |

---

## 📈 Key Features

* Real-time ingestion of simulated user events
* Stream processing with schema enforcement and JSON parsing
* Delta Lake storage with ACID compliance
* Aggregated analytics for top products, active users, and device types
* Interactive Tableau dashboard for real-time reporting

---

## 🧪 Project Flow

### 1. **Simulate User Events**

Generate random e-commerce events (clicks, views) using a Python script and publish them to a Kafka topic.

```python
from kafka import KafkaProducer
import json, time, random

producer = KafkaProducer(bootstrap_servers='localhost:9092',
                         value_serializer=lambda x: json.dumps(x).encode('utf-8'))

while True:
    message = {
        "user_id": random.randint(1, 50),
        "timestamp": time.time(),
        "event_type": random.choice(["click", "view"]),
        "product_id": random.randint(100, 110),
        "device_type": random.choice(["web", "mobile"])
    }
    producer.send("user_activity", value=message)
    time.sleep(0.5)
```

---

### 2. **Stream from Kafka in Databricks**

Use Spark Structured Streaming to consume, parse, and store messages from Kafka into Delta Lake.

```python
df_kafka = (spark.readStream
    .format("kafka")
    .option("kafka.bootstrap.servers", "localhost:9092")
    .option("subscribe", "user_activity")
    .load())

# Define schema and parse JSON
from pyspark.sql.types import StructType, StringType, DoubleType
schema = StructType().add("user_id", StringType()).add("timestamp", DoubleType()) \
    .add("event_type", StringType()).add("product_id", StringType()) \
    .add("device_type", StringType())

parsed_df = df_kafka.selectExpr("CAST(value AS STRING)") \
    .select(from_json(col("value"), schema).alias("data")).select("data.*")

# Store in Delta Lake
parsed_df.writeStream \
    .format("delta") \
    .option("checkpointLocation", "/mnt/checkpoints/user_activity") \
    .outputMode("append") \
    .start("/mnt/datalake/user_activity")
```

---

### 3. **Batch Aggregation & Output**

Create a daily job to aggregate the most viewed products.

```python
df = spark.read.format("delta").load("/mnt/datalake/user_activity")

top_products = df.filter("event_type = 'view'") \
    .groupBy("product_id") \
    .count() \
    .orderBy("count", ascending=False)

top_products.write.format("delta").mode("overwrite").save("/mnt/datalake/top_products")
```

---

### 4. **Visualize with Tableau**

Connect Tableau to the Delta table (or export to a SQL data warehouse) and create visual dashboards:

* **Top 10 Most Viewed Products**
* **User Activity by Device Type**
* **User Engagement Over Time**

---

## 🚀 Future Improvements

* Integrate Kafka Connect to pull from real databases
* Use Airflow to orchestrate batch jobs
* Deploy with Docker for portability
* Add alerting via Slack or email for high activity spikes

---

## 📂 Repository Structure

```
📁 kafka_producer/
   └── simulate_events.py
📁 databricks/
   ├── stream_kafka_to_delta.py
   └── aggregate_views.py
📁 dashboard/
   └── tableau_dashboard.twb
```

---

## 👨‍💻 Author

**Hareesh Chitipothu**
📧 [hareeshc198@gmail.com](mailto:hareeshc198@gmail.com)
🌐 [LinkedIn](https://www.linkedin.com/in/hareesh-chitti-10181998october/)
🐙 [GitHub](https://github.com/Hareesh1998)
